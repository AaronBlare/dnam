# @package _global_

data_type: harmonized
outcome: "Status"

project_name: dnam_${data_type}_multiclass_${outcome}_inference_${model_type}

seed: 101

# model_type: lightgbm
# model_type: tabnetpl
# model_type: catboost
model_type: xgboost
# ckpt_path: "${base_dir}/${data_type}/models/dnam_harmonized_multiclass_Status_trn_val_tst_${model_type}/runs/2022-03-25_23-05-28/epoch_588_best_0013.txt"
# ckpt_path: "${base_dir}/models/${model_type}/runs/2022-02-17_01-37-15/checkpoints/100.ckpt"
# ckpt_path: "${base_dir}/models/${model_type}/runs/2022-02-17_00-52-21/epoch_587.model"
ckpt_path: "${base_dir}/${data_type}/models/dnam_harmonized_multiclass_Status_trn_val_tst_${model_type}/runs/2022-03-25_23-14-14/epoch_368_best_0013.model"

dataset: "GSE152027"

debug: False
print_config: True
ignore_warnings: True
test_after_training: True

in_dim: 110137
out_dim: 2

base_dir: "E:/YandexDisk/Work/pydnameth/datasets/meta/tasks/GPL13534_Blood/Schizophrenia"
work_dir: "${base_dir}/${data_type}/models/${project_name}"
data_dir: "${base_dir}/${data_type}"

is_shap: False
shap_explainer: Tree

# Plot params
num_top_features: 10
num_examples: 10

# specify here default training configuration
defaults:
  - override /trainer: null # choose trainer from 'configs/trainer/'
  - override /model: null
  - override /datamodule: null
  - override /callbacks: none.yaml
  - override /logger: many_loggers.yaml # set logger here or use command line (e.g. `python run.py logger=wandb`)
  - override /hydra/hydra_logging: colorlog
  - override /hydra/job_logging: colorlog

datamodule:
  _target_: src.datamodules.dnam.DNAmDataModuleInference
  task: "binary"
  features_fn: "${data_dir}/cpgs/${in_dim}.xlsx"
  classes_fn: "${data_dir}/statuses/${out_dim}.xlsx"
  trn_val_fn: "${data_dir}/data_trn_val.pkl"
  inference_fn: "${data_dir}/data_tst_${dataset}.pkl"
  outcome: "Status"
  batch_size: 128
  num_workers: 0
  pin_memory: False
  imputation: "median"
