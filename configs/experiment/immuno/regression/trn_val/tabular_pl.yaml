# @package _global_

task: regression
data_type: immuno
# Available frameworks of models:
# stand_alone
# pytorch
model_framework: stand_alone # stand_alone pytorch
# Available types of models:
# elastic_net (stand_alone)
# xgboost (stand_alone)
# catboost (stand_alone)
# lightgbm (stand_alone)
# widedeep_tab_mlp (pytorch)
# widedeep_tab_resnet (pytorch)
# widedeep_tab_net (pytorch)
# widedeep_tab_transformer (pytorch)
# widedeep_ft_transformer (pytorch)
# widedeep_saint (pytorch)
# widedeep_tab_fastformer (pytorch)
# widedeep_tab_perceiver (pytorch)
# pytorch_tabular_autoint (pytorch)
# pytorch_tabular_tabnet (pytorch)
# pytorch_tabular_node (pytorch)
# pytorch_tabular_category_embedding (pytorch)
# pytorch_tabular_ft_transformer (pytorch)
# pytorch_tabular_tab_transformer (pytorch)
model_type: catboost
target: Age

project_name: ${data_type}_trn_val_${model_type}

seed: 1337

in_dim: 1
out_dim: 1

cv_is_split: True
cv_n_splits: 5
cv_n_repeats: 1

optimized_metric: mean_absolute_error
optimized_mean: cv_mean
optimized_part: val # tst
direction: min

embed_dim: 16

is_shuffle: False

debug: False
print_config: True
ignore_warnings: True
test_after_training: True

max_epochs: 20
patience: 2

base_dir: "E:/YandexDisk/Work/pydnameth/datasets/GPL21145/GSEUNN/special/021_ml_data/${data_type}"
work_dir: "${base_dir}/models/${project_name}"
data_dir: "${base_dir}"

# SHAP values
is_shap: True # True False
is_shap_save: False
shap_explainer: Kernel # Tree Kernel Deep
shap_bkgrd: trn # trn all tree_path_dependent

# LIME weights
is_lime: False # True False
lime_bkgrd: trn # trn all
lime_num_features: all # 10 all
lime_save_weights: True

# Plot params
num_top_features: 10
num_examples: 10

# specify here default training configuration
defaults:
  - _self_
  - override /datamodule: null
  - override /trainer: default.yaml # choose trainer from 'configs/trainer/'
  - override /model: null
  - override /callbacks: regular.yaml
  - override /logger: many_loggers.yaml # set logger here or use command line (e.g. `python run.py logger=wandb`)
  - override /hydra/hydra_logging: colorlog
  - override /hydra/job_logging: colorlog

datamodule:
  _target_: src.datamodules.tabular.TabularDataModule
  task: ${task}
  feats_con_fn: "${data_dir}/feats_con.xlsx"
  feats_cat_fn: null
  feats_cat_encoding: label
  feats_cat_embed_dim: ${embed_dim}
  target: ${target}
  target_classes_fn: null
  data_fn: "${data_dir}/260_imp(fast_knn)_replace(quarter).xlsx"
  data_index: index
  data_imputation: fast_knn
  split_by: trn_val
  split_trn_val: [0.80, 0.20]
  split_top_feat: null
  split_explicit_feat: Split
  batch_size: 64
  num_workers: 0
  pin_memory: False
  seed: ${seed}
  weighted_sampler: True

trainer:
  gpus: 1
  min_epochs: 1
  max_epochs: ${max_epochs}
  weights_summary: null
  progress_bar_refresh_rate: 10 #10
  resume_from_checkpoint: null

model:
  type: ${model_type}

xgboost:
  booster: 'gbtree'
  learning_rate: 0.05
  max_depth: 5
  gamma: 0
  sampling_method: 'uniform'
  subsample: 1
  objective: 'reg:squarederror'
  verbosity: 0
  eval_metric: 'mae'
  max_epochs: ${max_epochs}
  patience: ${patience}

catboost:
  loss_function: 'MAE'
  learning_rate: 0.03250338619913498
  depth: 4
  min_data_in_leaf: 35
  max_leaves: 31
  task_type: 'CPU'
  verbose: 0
  max_epochs: ${max_epochs}
  patience: ${patience}

lightgbm:
  objective: 'regression'
  boosting: 'gbdt'
  learning_rate: 0.05
  num_leaves: 15
  device: 'cpu'
  max_depth: -1
  min_data_in_leaf: 7
  feature_fraction: 0.9
  bagging_fraction: 0.7
  bagging_freq: 5
  verbose: -1
  metric: 'l1'
  max_epochs: ${max_epochs}
  patience: ${patience}

elastic_net:
  alpha: 1.122
  l1_ratio: 0.5
  max_iter: 100000
  tol: 1e-2

widedeep_tab_mlp:
  _target_: src.models.tabular.widedeep.tab_mlp.WDTabMLPModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.01
  optimizer_weight_decay: 0.0
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  column_idx: null
  cat_embed_input: null
  cat_embed_dropout: 0.1
  use_cat_bias: False
  cat_embed_activation: null
  continuous_cols: null
  cont_norm_layer: "batchnorm"
  embed_continuous: False
  cont_embed_dim: 32
  cont_embed_dropout: 0.1
  use_cont_bias: True
  cont_embed_activation: null
  mlp_hidden_dims:
  - 200
  - 100
  - ${out_dim}
  mlp_activation: 'relu'
  mlp_dropout: 0.1
  mlp_batchnorm: False
  mlp_batchnorm_last: False
  mlp_linear_first: False

widedeep_tab_resnet:
  _target_: src.models.tabular.widedeep.tab_resnet.WDTabResnetModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.07
  optimizer_weight_decay: 0.0
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  column_idx: null
  cat_embed_input: null
  cat_embed_dropout: 0.1
  use_cat_bias: False
  cat_embed_activation: null
  continuous_cols: null
  cont_norm_layer: "batchnorm"
  embed_continuous: False
  cont_embed_dim: 32
  cont_embed_dropout: 0.1
  use_cont_bias: True
  cont_embed_activation: null
  blocks_dims:
  - 200
  - 100
  - 100
  - ${out_dim}
  blocks_dropout: 0.1
  simplify_blocks: False
  mlp_hidden_dims: null
  mlp_activation: 'relu'
  mlp_dropout: 0.1
  mlp_batchnorm: False
  mlp_batchnorm_last: False
  mlp_linear_first: False
  
widedeep_tab_net:
  _target_: src.models.tabular.widedeep.tab_net.WDTabNetModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.01
  optimizer_weight_decay: 0.0
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  column_idx: null
  cat_embed_input: null
  cat_embed_dropout: 0.1
  use_cat_bias: False
  cat_embed_activation: null
  continuous_cols: null
  cont_norm_layer: "batchnorm"
  embed_continuous: False
  cont_embed_dim: 32
  cont_embed_dropout: 0.1
  use_cont_bias: True
  cont_embed_activation: null
  n_steps: 3
  attn_dim: 8
  dropout: 0.0
  n_glu_step_dependent: 2
  n_glu_shared: 2
  ghost_bn: True
  virtual_batch_size: 128
  momentum: 0.02
  gamma: 1.3
  epsilon: 1e-15
  mask_type: "sparsemax"

widedeep_tab_transformer:
  _target_: src.models.tabular.widedeep.tab_transformer.WDTabTransformerModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.001
  optimizer_weight_decay: 0.0
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  column_idx: null
  cat_embed_input: null
  cat_embed_dropout: 0.1
  use_cat_bias: False
  cat_embed_activation: null
  full_embed_dropout: False
  shared_embed: False
  add_shared_embed: False
  frac_shared_embed: 0.25
  continuous_cols: null
  cont_norm_layer: null
  embed_continuous: True
  cont_embed_dropout: 0.1
  use_cont_bias: True
  cont_embed_activation: null
  embed_dim: ${embed_dim}
  n_heads: 8
  use_qkv_bias: False
  n_blocks: 4
  attn_dropout: 0.2
  ff_dropout: 0.1
  transformer_activation: "gelu"
  mlp_hidden_dims:
    - 200
    - 100
    - ${out_dim}
  mlp_activation: "relu"
  mlp_dropout: 0.1
  mlp_batchnorm: False
  mlp_batchnorm_last: False
  mlp_linear_first: True

widedeep_ft_transformer:
  _target_: src.models.tabular.widedeep.ft_transformer.WDFTTransformerModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.0005
  optimizer_weight_decay: 0.0
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  column_idx: null
  cat_embed_input: null
  cat_embed_dropout: 0.1
  use_cat_bias: False
  cat_embed_activation: null
  full_embed_dropout: False
  shared_embed: False
  add_shared_embed: False
  frac_shared_embed: 0.25
  continuous_cols: null
  cont_norm_layer: null
  cont_embed_dropout: 0.1
  use_cont_bias: True
  cont_embed_activation: null
  embed_dim: 64
  kv_compression_factor: 0.5
  kv_sharing: False
  use_qkv_bias: False
  n_heads: 8
  n_blocks: 4
  attn_dropout: 0.2
  ff_dropout: 0.1
  transformer_activation: "reglu"
  ff_factor: 1.33
  mlp_hidden_dims:
    - 200
    - 100
    - ${out_dim}
  mlp_activation: "relu"
  mlp_dropout: 0.1
  mlp_batchnorm: False
  mlp_batchnorm_last: False
  mlp_linear_first: True

widedeep_saint:
  _target_: src.models.tabular.widedeep.saint.WDSAINTModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.001
  optimizer_weight_decay: 0.0
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  column_idx: null
  cat_embed_input: null
  cat_embed_dropout: 0.1
  use_cat_bias: False
  cat_embed_activation: null
  full_embed_dropout: False
  shared_embed: False
  add_shared_embed: False
  frac_shared_embed: 0.25
  continuous_cols: null
  cont_norm_layer: null
  cont_embed_dropout: 0.1
  use_cont_bias: True
  cont_embed_activation: null
  embed_dim: 4
  use_qkv_bias: False
  n_heads: 2
  n_blocks: 1
  attn_dropout: 0.1
  ff_dropout: 0.2
  transformer_activation: "gelu"
  mlp_hidden_dims:
    - 200
    - 100
    - ${out_dim}
  mlp_activation: "relu"
  mlp_dropout: 0.1
  mlp_batchnorm: False
  mlp_batchnorm_last: False
  mlp_linear_first: True

widedeep_tab_fastformer:
  _target_: src.models.tabular.widedeep.tab_fastformer.WDTabFastFormerModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.001
  optimizer_weight_decay: 0.0
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  column_idx: null
  cat_embed_input: null
  cat_embed_dropout: 0.1
  use_cat_bias: False
  cat_embed_activation: null
  full_embed_dropout: False
  shared_embed: False
  add_shared_embed: False
  frac_shared_embed: 0.25
  continuous_cols: null
  cont_norm_layer: null
  cont_embed_dropout: 0.1
  use_cont_bias: True
  cont_embed_activation: null
  embed_dim: ${embed_dim}
  n_heads: 8
  use_bias: False
  n_blocks: 4
  attn_dropout: 0.2
  ff_dropout: 0.1
  share_qv_weights: False,
  share_weights: False,
  transformer_activation: "relu"
  mlp_hidden_dims:
    - 100
    - ${out_dim}
  mlp_activation: "relu"
  mlp_dropout: 0.1
  mlp_batchnorm: False
  mlp_batchnorm_last: False
  mlp_linear_first: True

widedeep_tab_perceiver:
  _target_: src.models.tabular.widedeep.tab_perceiver.WDTabPerceiverModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.001
  optimizer_weight_decay: 0.0
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  column_idx: null
  cat_embed_input: null
  cat_embed_dropout: 0.1
  use_cat_bias: False
  cat_embed_activation: null
  full_embed_dropout: False
  shared_embed: False
  add_shared_embed: False
  frac_shared_embed: 0.25
  continuous_cols: null
  cont_norm_layer: null
  cont_embed_dropout: 0.1
  use_cont_bias: True
  cont_embed_activation: null
  embed_dim: ${embed_dim}
  n_cross_attns: 1
  n_cross_attn_heads:  4
  n_latents: 16
  latent_dim: 128
  n_latent_heads: 4
  n_latent_blocks: 4
  n_perceiver_blocks: 4
  share_weights: False
  attn_dropout: 0.1
  ff_dropout: 0.1
  transformer_activation: "geglu"
  mlp_hidden_dims:
    - 100
    - ${out_dim}
  mlp_activation: "relu"
  mlp_dropout: 0.1
  mlp_batchnorm: False
  mlp_batchnorm_last: False
  mlp_linear_first: True

pytorch_tabular_autoint:
  _target_: src.models.tabular.pytorch_tabular.autoint.PTAutoIntModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.0005
  optimizer_weight_decay: 0.0
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  embedding_dims: null
  continuous_cols: null
  categorical_cols: null
  attn_embed_dim: 32
  num_heads: 2
  num_attn_blocks: 3
  attn_dropouts: 0.0
  has_residuals: True
  embedding_dim: 16
  embedding_dropout: 0.0
  deep_layers: False
  layers: "128-64-32"
  activation: "ReLU"
  dropout: 0.0
  use_batch_norm: False
  batch_norm_continuous_input: False
  attention_pooling: False
  initialization: "kaiming"

pytorch_tabular_tabnet:
  _target_: src.models.tabular.pytorch_tabular.tabnet.PTTabNetModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.01
  optimizer_weight_decay: 0.0
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  embedding_dims: null
  continuous_cols: null
  categorical_cols: null
  n_d: 8
  n_a: 8
  n_steps: 3
  gamma: 1.3
  n_independent: 1
  n_shared: 2
  virtual_batch_size: 128
  mask_type: "sparsemax"
  
pytorch_tabular_node:
  _target_: src.models.tabular.pytorch_tabular.node.PTNODEModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.2
  optimizer_weight_decay: 0.0
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  embedding_dims: null
  continuous_cols: null
  categorical_cols: null
  num_layers: 1
  num_trees: 2048
  additional_tree_output_dim: 3
  depth: 6
  choice_function: "entmax15"
  bin_function: "entmoid15"
  max_features: null
  input_dropout: 0.0
  initialize_response: "normal"
  initialize_selection_logits: "uniform"
  threshold_init_beta: 1.0
  threshold_init_cutoff: 1.0
  embed_categorical: False
  embedding_dropout: 0.0

pytorch_tabular_category_embedding:
  _target_: src.models.tabular.pytorch_tabular.category_embedding.PTCategoryEmbeddingModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.05
  optimizer_weight_decay: 0.0
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  embedding_dims: null
  continuous_cols: null
  categorical_cols: null
  layers: "128-64-32"
  batch_norm_continuous_input: True
  activation: "ReLU"
  embedding_dropout: 0.5
  dropout: 0.5
  use_batch_norm: False
  initialization: "kaiming"

pytorch_tabular_ft_transformer:
  _target_: src.models.tabular.pytorch_tabular.ft_transformer.PTFTTransformerModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.05
  optimizer_weight_decay: 0.0
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  embedding_dims: null
  continuous_cols: null
  categorical_cols: null
  input_embed_dim: 32
  embedding_initialization: "kaiming_uniform"
  embedding_bias: True
  embedding_dropout: 0.1
  share_embedding: False
  share_embedding_strategy: "fraction"
  shared_embedding_fraction: 0.25
  attn_feature_importance: True
  num_heads: 8
  num_attn_blocks: 6
  transformer_head_dim: null
  attn_dropout: 0.1
  add_norm_dropout: 0.1
  ff_dropout: 0.1
  ff_hidden_multiplier: 4
  transformer_activation: "GEGLU"
  out_ff_layers: "128-64-32"
  out_ff_activation: "ReLU"
  out_ff_dropout: 0.0
  use_batch_norm: False
  batch_norm_continuous_input: False
  out_ff_initialization: "kaiming"

pytorch_tabular_tab_transformer:
  _target_: src.models.tabular.pytorch_tabular.tab_transformer.PTTabTransformerModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.05
  optimizer_weight_decay: 0.0
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  embedding_dims: null
  continuous_cols: null
  categorical_cols: null
  input_embed_dim: 32
  embedding_dropout: 0.1
  share_embedding: False
  share_embedding_strategy: "fraction"
  shared_embedding_fraction: 0.25
  num_heads: 8
  num_attn_blocks: 6
  transformer_head_dim: null
  attn_dropout: 0.1
  add_norm_dropout: 0.1
  ff_dropout: 0.1
  ff_hidden_multiplier: 4
  transformer_activation: "GEGLU"
  out_ff_layers: "128-64-32"
  out_ff_activation: "ReLU"
  out_ff_dropout: 0.0
  use_batch_norm: False
  batch_norm_continuous_input: False
  out_ff_initialization: "kaiming"

callbacks:
  model_checkpoint:
    monitor: "val/${optimized_metric}_pl" # name of the logged metric which determines when model is improving
    mode: ${direction} # can be "max" or "min"
    save_top_k: 1 # save k best models (determined by above metric)
    save_last: False # additionaly always save model from last epoch
    verbose: False
    dirpath: ""
    filename: "best"
    auto_insert_metric_name: False

  early_stopping:
    monitor: "val/${optimized_metric}_pl" # name of the logged metric which determines when model is improving
    mode: ${direction} # can be "max" or "min"
    patience: ${patience} # how many epochs of not improving until training stops
    min_delta: 0 # minimum change in the monitored metric needed to qualify as an improvement
